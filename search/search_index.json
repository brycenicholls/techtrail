{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ABOUT ME tail -f /var/log/career I am a Cloud Native Infrastructure Engineer with experience in various domains in the IT and Telecommunication Industry. I have experience working in SC cleared cloud environments. My predisposition to take things apart and see how they work began as a child. This has evolved today into a fascination for all things pertaining to technology. Experience Openstack Infrastructure Engineer | (THG) July 2022 - Current Administering four openstack regions running Kolla-Ansible Openstack. Maintaining the Ceph platforms including upgrades and disk replacements. Kernel upgrades and patching to the regions using ansible. Openstack Infrastructure Engineer | (UKCloud) May 2018 - July 2022 Administering two openstack regions running RedHat TripleO using Ansible and Python. Performing upgrades to the openstack platform. Openstack platform scaleouts, adding compute nodes. Maintaining the CEPH storage clusters and performing updates / fixes. Ceph storage cluster scaleouts. Ceph disk replacements. Troubleshooting issues on the Openstack and Ceph clusters. Senior Cloud Support Engineer | (UKCloud) May 2015 - May 2018 Provisioning openstack projects and helping setup customer environments. Customer escalations on the openstack and vmware platform. Providing training to the support and NOC teams. Troubleshooting customer network issues and implementing cisco firewall and routing changes on the infrastructure to allow customer access and routing. Improvements to the triage process for the pport teams. Technical Service Desk Network Engineer | (Easynet) March 2013 - May 2015 Technical support / Customer service. Ticket Management. Fault diagnostics / Escalations. Service restoration. HSRP failover tests. Router configuration changes. Proactive network monitoring. IT Support Technician | (Sommerset College) Jan 2013 - March 2013 Supported the college IT team by keeping the infrastructure up and running. Second Line Desktop support | (Comet) Oct 2012 - Dec 2012 Provided telephone and remote support to end users who bought computers and tablets at comet. Desktop Technician | (QSS Systems) Apr 2010 - Oct 2012 Telephone and on-site IT support for local buisnesses and schools. Troubleshooting Windows and Mac computers. Setting up and maintaining small office networks.","title":"Home"},{"location":"#about-me","text":"tail -f /var/log/career I am a Cloud Native Infrastructure Engineer with experience in various domains in the IT and Telecommunication Industry. I have experience working in SC cleared cloud environments. My predisposition to take things apart and see how they work began as a child. This has evolved today into a fascination for all things pertaining to technology.","title":"ABOUT ME"},{"location":"#experience","text":"","title":"Experience"},{"location":"#openstack-infrastructure-engineer-thg-july-2022-current","text":"Administering four openstack regions running Kolla-Ansible Openstack. Maintaining the Ceph platforms including upgrades and disk replacements. Kernel upgrades and patching to the regions using ansible.","title":"Openstack Infrastructure Engineer | (THG) July 2022 - Current"},{"location":"#openstack-infrastructure-engineer-ukcloud-may-2018-july-2022","text":"Administering two openstack regions running RedHat TripleO using Ansible and Python. Performing upgrades to the openstack platform. Openstack platform scaleouts, adding compute nodes. Maintaining the CEPH storage clusters and performing updates / fixes. Ceph storage cluster scaleouts. Ceph disk replacements. Troubleshooting issues on the Openstack and Ceph clusters.","title":"Openstack Infrastructure Engineer | (UKCloud) May 2018 - July 2022"},{"location":"#senior-cloud-support-engineer-ukcloud-may-2015-may-2018","text":"Provisioning openstack projects and helping setup customer environments. Customer escalations on the openstack and vmware platform. Providing training to the support and NOC teams. Troubleshooting customer network issues and implementing cisco firewall and routing changes on the infrastructure to allow customer access and routing. Improvements to the triage process for the pport teams.","title":"Senior Cloud Support Engineer | (UKCloud) May 2015 - May 2018"},{"location":"#technical-service-desk-network-engineer-easynet-march-2013-may-2015","text":"Technical support / Customer service. Ticket Management. Fault diagnostics / Escalations. Service restoration. HSRP failover tests. Router configuration changes. Proactive network monitoring.","title":"Technical Service Desk Network Engineer | (Easynet) March 2013 - May 2015"},{"location":"#it-support-technician-sommerset-college-jan-2013-march-2013","text":"Supported the college IT team by keeping the infrastructure up and running.","title":"IT Support Technician | (Sommerset College) Jan 2013 - March 2013"},{"location":"#second-line-desktop-support-comet-oct-2012-dec-2012","text":"Provided telephone and remote support to end users who bought computers and tablets at comet.","title":"Second Line Desktop support | (Comet) Oct 2012 - Dec 2012"},{"location":"#desktop-technician-qss-systems-apr-2010-oct-2012","text":"Telephone and on-site IT support for local buisnesses and schools. Troubleshooting Windows and Mac computers. Setting up and maintaining small office networks.","title":"Desktop Technician | (QSS Systems) Apr 2010 - Oct 2012"},{"location":"ceph/ceph-inconsistent-pgs/","text":"CEPH PGS INCONSISTENT Sometimes you come across issues with the ceph placement groups where you see inconsistent pg errors. This normally indicates a potential issue with the osd. The below steps will help fix the error and identify the osd / disk that may be erroring. Run a ceph status command to verify the error. Here you can see 1 pg is inconsistent. [root@ceph_monitor_1 ~]# ceph -s cluster 46268b52-5b0e-11e6-8fac-525400e2a6d8 health HEALTH_ERR 1 pgs inconsistent 1 scrub errors monmap e1: 3 mons at {ceph_monitor_1=10.10.1.1:6789/0,ceph_monitor_2=10.10.1.2:6789/0,ceph_monitor_3=10.10.1.3:6789/0} election epoch 1418, quorum 0,1,2 ceph_monitor_2,ceph_monitor_1,ceph_monitor_3 osdmap e262059: 568 osds: 568 up, 568 in flags sortbitwise,require_jewel_osds pgmap v105732057: 25664 pgs, 7 pools, 113 TB data, 33762 kobjects 346 TB used, 415 TB / 762 TB avail 25660 active+clean 3 active+clean+scrubbing+deep 1 active+clean+inconsistent client io 8657 kB/s rd, 34724 kB/s wr, 1373 op/s rd, 4926 op/s wr To identify which pg has the error run \u2018ceph health detail\u2019. In this case we can see that pg 5.291b is the pg that has the issue. We can also identify the OSDs that the pg is part of. [130,496,354] [root@ceph_monitor_1 ~]# ceph health detail HEALTH_ERR 1 pgs inconsistent; 1 scrub errors; pool metrics has many more objects per pg than average (too few pgs?) pg 5.291b is active+clean+inconsistent, acting [130,496,354] 1 scrub errors pool metrics objects per pg (17848) is more than 13.2502 times cluster average (1347) We issue the command to repair the pg \u2018 ceph pg repair \u2018 NOTE: The output will identify the affected OSD that is being repaired. [root@ceph_monitor_1 ~]# ceph pg repair 5.291b instructing pg 5.291b on osd.130 to repair If you have a second session running on the ceph monitor, you can see the logs where the pg is being repaired. This is done by issuing the \u2018ceph health\u2019 or \u2018ceph -h\u2019 command as below. [root@ceph_monitor_1 ~]# ceph -w cluster 46268b52-5b0e-11e6-8fac-525400e2a6d8 health HEALTH_ERR 1 pgs inconsistent 1 scrub errors monmap e1: 3 mons at {ceph_monitor_1=10.10.1.1:6789/0,ceph_monitor_2=10.10.1.2:6789/0,ceph_monitor_3=10.10.1.3:6789/0} election epoch 1418, quorum 0,1,2 ceph_monitor_2,ceph_monitor_1,ceph_monitor_3 osdmap e262059: 568 osds: 568 up, 568 in flags sortbitwise,require_jewel_osds pgmap v105732231: 25664 pgs, 7 pools, 113 TB data, 33762 kobjects 346 TB used, 415 TB / 762 TB avail 25660 active+clean 3 active+clean+scrubbing+deep 1 active+clean+inconsistent client io 8482 kB/s rd, 112 MB/s wr, 334 op/s rd, 5012 op/s wr <..> 2020-02-05 10:57:11.552607 mon.1 [INF] from='client.? 10.10.1.1:0/4080461660' entity='client.admin' cmd=[{\"prefix\": \"pg repair\", \"pgid\": \"5.291b\"}]: dispatch 2020-02-05 10:57:11.994610 mon.0 [INF] pgmap v105732235: 25664 pgs: 3 active+clean+scrubbing+deep, 1 active+clean+inconsistent, 25660 active+clean; 113 TB data, 346 TB used, 415 TB / 762 TB avail; 1995 kB/s rd, 42833 kB/s wr, 2095 op/s 2020-02-05 10:57:12.135573 osd.130 [INF] 5.291b repair starts <..> 2020-02-05 10:57:44.379026 osd.130 [ERR] 5.291b shard 130: soid 5:d895133f:::rbd_data.b2d42a650cc047.0000000000000f58:head candidate had a read error <..> 2020-02-05 10:58:18.635846 osd.130 [ERR] 5.291b repair 0 missing, 1 inconsistent objects 2020-02-05 10:58:18.637245 osd.130 [ERR] 5.291b repair 1 errors, 1 fixed To identify which storage node the affected OSD is part of run the ceph osd find <OSD> command. Here I have narrowed it down to just the host name. [root@ceph_monitor_1 ~]# ceph osd find 130 | grep host \"host\": \"STOR00010\",","title":"Inconsistent PGs"},{"location":"ceph/ceph-inconsistent-pgs/#ceph-pgs-inconsistent","text":"Sometimes you come across issues with the ceph placement groups where you see inconsistent pg errors. This normally indicates a potential issue with the osd. The below steps will help fix the error and identify the osd / disk that may be erroring. Run a ceph status command to verify the error. Here you can see 1 pg is inconsistent. [root@ceph_monitor_1 ~]# ceph -s cluster 46268b52-5b0e-11e6-8fac-525400e2a6d8 health HEALTH_ERR 1 pgs inconsistent 1 scrub errors monmap e1: 3 mons at {ceph_monitor_1=10.10.1.1:6789/0,ceph_monitor_2=10.10.1.2:6789/0,ceph_monitor_3=10.10.1.3:6789/0} election epoch 1418, quorum 0,1,2 ceph_monitor_2,ceph_monitor_1,ceph_monitor_3 osdmap e262059: 568 osds: 568 up, 568 in flags sortbitwise,require_jewel_osds pgmap v105732057: 25664 pgs, 7 pools, 113 TB data, 33762 kobjects 346 TB used, 415 TB / 762 TB avail 25660 active+clean 3 active+clean+scrubbing+deep 1 active+clean+inconsistent client io 8657 kB/s rd, 34724 kB/s wr, 1373 op/s rd, 4926 op/s wr To identify which pg has the error run \u2018ceph health detail\u2019. In this case we can see that pg 5.291b is the pg that has the issue. We can also identify the OSDs that the pg is part of. [130,496,354] [root@ceph_monitor_1 ~]# ceph health detail HEALTH_ERR 1 pgs inconsistent; 1 scrub errors; pool metrics has many more objects per pg than average (too few pgs?) pg 5.291b is active+clean+inconsistent, acting [130,496,354] 1 scrub errors pool metrics objects per pg (17848) is more than 13.2502 times cluster average (1347) We issue the command to repair the pg \u2018 ceph pg repair \u2018 NOTE: The output will identify the affected OSD that is being repaired. [root@ceph_monitor_1 ~]# ceph pg repair 5.291b instructing pg 5.291b on osd.130 to repair If you have a second session running on the ceph monitor, you can see the logs where the pg is being repaired. This is done by issuing the \u2018ceph health\u2019 or \u2018ceph -h\u2019 command as below. [root@ceph_monitor_1 ~]# ceph -w cluster 46268b52-5b0e-11e6-8fac-525400e2a6d8 health HEALTH_ERR 1 pgs inconsistent 1 scrub errors monmap e1: 3 mons at {ceph_monitor_1=10.10.1.1:6789/0,ceph_monitor_2=10.10.1.2:6789/0,ceph_monitor_3=10.10.1.3:6789/0} election epoch 1418, quorum 0,1,2 ceph_monitor_2,ceph_monitor_1,ceph_monitor_3 osdmap e262059: 568 osds: 568 up, 568 in flags sortbitwise,require_jewel_osds pgmap v105732231: 25664 pgs, 7 pools, 113 TB data, 33762 kobjects 346 TB used, 415 TB / 762 TB avail 25660 active+clean 3 active+clean+scrubbing+deep 1 active+clean+inconsistent client io 8482 kB/s rd, 112 MB/s wr, 334 op/s rd, 5012 op/s wr <..> 2020-02-05 10:57:11.552607 mon.1 [INF] from='client.? 10.10.1.1:0/4080461660' entity='client.admin' cmd=[{\"prefix\": \"pg repair\", \"pgid\": \"5.291b\"}]: dispatch 2020-02-05 10:57:11.994610 mon.0 [INF] pgmap v105732235: 25664 pgs: 3 active+clean+scrubbing+deep, 1 active+clean+inconsistent, 25660 active+clean; 113 TB data, 346 TB used, 415 TB / 762 TB avail; 1995 kB/s rd, 42833 kB/s wr, 2095 op/s 2020-02-05 10:57:12.135573 osd.130 [INF] 5.291b repair starts <..> 2020-02-05 10:57:44.379026 osd.130 [ERR] 5.291b shard 130: soid 5:d895133f:::rbd_data.b2d42a650cc047.0000000000000f58:head candidate had a read error <..> 2020-02-05 10:58:18.635846 osd.130 [ERR] 5.291b repair 0 missing, 1 inconsistent objects 2020-02-05 10:58:18.637245 osd.130 [ERR] 5.291b repair 1 errors, 1 fixed To identify which storage node the affected OSD is part of run the ceph osd find <OSD> command. Here I have narrowed it down to just the host name. [root@ceph_monitor_1 ~]# ceph osd find 130 | grep host \"host\": \"STOR00010\",","title":"CEPH PGS INCONSISTENT"},{"location":"kubernetes/command-cheatsheet/","text":"Command Cheatsheet create pod cli: kubectl run {pod name} --image={image name} generate yaml pod definition template: kubectl run {pod name} --image={image name} --dry-run -o yaml replica set: kubectl create -f {file name} kubectl get replicaset kubectl delete replicaset {name} kubectl replace -f { file name} kubectl scale replicaset --replicas={number} {replicaset name}","title":"Command Cheatsheet"},{"location":"kubernetes/command-cheatsheet/#command-cheatsheet","text":"create pod cli: kubectl run {pod name} --image={image name} generate yaml pod definition template: kubectl run {pod name} --image={image name} --dry-run -o yaml replica set: kubectl create -f {file name} kubectl get replicaset kubectl delete replicaset {name} kubectl replace -f { file name} kubectl scale replicaset --replicas={number} {replicaset name}","title":"Command Cheatsheet"},{"location":"openstack/ost-client/","text":"INSTALLING THE OPENSTACK CLIENT In order to interact with the openstack API you will need to install the client. In this example, I am going to install the client in a python virtual environment. This gives me the option of installing and switching between different versions of the client. PREREQUISITES Warning The openstack client only supports python 2.7 at the moment. sudo yum -y install epel-release sudo yum -y install python python-devel python-pip gcc sudo pip install --upgrade pip sudo pip install virtualenv Once the packages are installed, create a folder for your virtual environments. I am naming the folder \u2018venvs\u2019 and changing into that directory. mkdir venvs && cd venvs Create the virtual environment [centos@osp-client venvs]$ python -m virtualenv openstack_client No LICENSE.txt / LICENSE found in source New python executable in /home/centos/venvs/openstack_client/bin/python2 Also creating executable in /home/centos/venvs/openstack_client/bin/python Installing setuptools, pip, wheel... done. [centos@osp-client venvs]$ ll total 0 drwxrwxr-x. 5 centos centos 56 Jan 28 12:47 openstack_client Activate the virtual environment by running the below command. Notice that the prompt changes once activated. I will create a separate post on how you can configure your bash and zsh environments to give you more features and functionality. [centos@osp-client venvs]$ source openstack_client/bin/activate (openstack_client) [centos@osp-client venvs]$ Install the all-in-one client (openstack_client) [centos@osp-client venvs]$ pip install python-openstackclient To deactivate the virtual environment just type \u2018deactivate\u2019. You will notice that the prompt changes again once the virtual environment is deactivated. (openstack_client) [centos@osp-client venvs]$ deactivate [centos@osp-client venvs]$ Using the above process, you can create multiple virtual environments and install different versions of the client as per your requirements.","title":"Openstack Client"},{"location":"openstack/ost-client/#installing-the-openstack-client","text":"In order to interact with the openstack API you will need to install the client. In this example, I am going to install the client in a python virtual environment. This gives me the option of installing and switching between different versions of the client.","title":"INSTALLING THE OPENSTACK CLIENT"},{"location":"openstack/ost-client/#prerequisites","text":"Warning The openstack client only supports python 2.7 at the moment. sudo yum -y install epel-release sudo yum -y install python python-devel python-pip gcc sudo pip install --upgrade pip sudo pip install virtualenv Once the packages are installed, create a folder for your virtual environments. I am naming the folder \u2018venvs\u2019 and changing into that directory. mkdir venvs && cd venvs","title":"PREREQUISITES"},{"location":"openstack/ost-client/#create-the-virtual-environment","text":"[centos@osp-client venvs]$ python -m virtualenv openstack_client No LICENSE.txt / LICENSE found in source New python executable in /home/centos/venvs/openstack_client/bin/python2 Also creating executable in /home/centos/venvs/openstack_client/bin/python Installing setuptools, pip, wheel... done. [centos@osp-client venvs]$ ll total 0 drwxrwxr-x. 5 centos centos 56 Jan 28 12:47 openstack_client Activate the virtual environment by running the below command. Notice that the prompt changes once activated. I will create a separate post on how you can configure your bash and zsh environments to give you more features and functionality. [centos@osp-client venvs]$ source openstack_client/bin/activate (openstack_client) [centos@osp-client venvs]$","title":"Create the virtual environment"},{"location":"openstack/ost-client/#install-the-all-in-one-client","text":"(openstack_client) [centos@osp-client venvs]$ pip install python-openstackclient To deactivate the virtual environment just type \u2018deactivate\u2019. You will notice that the prompt changes again once the virtual environment is deactivated. (openstack_client) [centos@osp-client venvs]$ deactivate [centos@osp-client venvs]$ Using the above process, you can create multiple virtual environments and install different versions of the client as per your requirements.","title":"Install the all-in-one client"}]}